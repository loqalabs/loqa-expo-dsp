{"version":3,"file":"types.js","sourceRoot":"","sources":["../src/types.ts"],"names":[],"mappings":"AAAA,0CAA0C","sourcesContent":["// Type definitions for LoqaExpoDsp module\n\n/**\n * Configuration options for FFT computation\n *\n * @example\n * ```typescript\n * const options: FFTOptions = {\n *   fftSize: 2048,           // Power of 2 between 256-8192\n *   windowType: 'hanning',   // Reduces spectral leakage\n *   includePhase: false      // Omit phase for performance\n * };\n * ```\n */\nexport interface FFTOptions {\n  /**\n   * FFT size (must be power of 2, range: 256-8192).\n   * Defaults to buffer length. Larger sizes provide better frequency resolution\n   * but lower time resolution.\n   */\n  fftSize?: number;\n  /**\n   * Window function type. Defaults to 'hanning'.\n   * - 'hanning': Good general-purpose window (default)\n   * - 'hamming': Similar to Hanning, slightly different sidelobe behavior\n   * - 'blackman': Better frequency resolution, more attenuation\n   * - 'none': Rectangular window (use only for perfect sine waves)\n   */\n  windowType?: 'hanning' | 'hamming' | 'blackman' | 'none';\n  /**\n   * Return phase information. Defaults to false.\n   * Set to true only if you need phase data, as it increases computation time.\n   */\n  includePhase?: boolean;\n}\n\n/**\n * Result of FFT computation\n *\n * Contains frequency-domain representation of the input audio signal.\n *\n * @example\n * ```typescript\n * const result = await computeFFT(audioBuffer, { fftSize: 2048 });\n *\n * // Find peak frequency\n * const peakIndex = result.magnitude.indexOf(Math.max(...result.magnitude));\n * const peakFrequency = result.frequencies[peakIndex];\n * console.log(`Peak at ${peakFrequency} Hz`);\n * ```\n */\nexport interface FFTResult {\n  /**\n   * Magnitude spectrum (length = fftSize / 2)\n   *\n   * Each element represents the amplitude at the corresponding frequency.\n   * Higher values indicate stronger presence of that frequency component.\n   */\n  magnitude: Float32Array;\n  /**\n   * Phase spectrum (only present if includePhase: true)\n   *\n   * Each element represents the phase angle in radians at the corresponding frequency.\n   * Useful for signal reconstruction and phase-based analysis.\n   */\n  phase?: Float32Array;\n  /**\n   * Frequency bin centers in Hz\n   *\n   * Each element corresponds to the center frequency of each magnitude/phase bin.\n   * Use this array to map magnitude values to their frequencies.\n   */\n  frequencies: Float32Array;\n}\n\n/**\n * Configuration options for pitch detection\n */\nexport interface PitchDetectionOptions {\n  /** Sample rate in Hz */\n  sampleRate: number;\n  /** Minimum detectable frequency in Hz. Defaults to 80. */\n  minFrequency?: number;\n  /** Maximum detectable frequency in Hz. Defaults to 400. */\n  maxFrequency?: number;\n}\n\n/**\n * Result of pitch detection\n *\n * @example\n * ```typescript\n * const result = await detectPitch(audioData, 44100);\n *\n * if (result.isVoiced) {\n *   console.log(`Pitch: ${result.frequency} Hz`);\n *   console.log(`Confidence: ${result.confidence}`);\n *   console.log(`Voiced probability: ${result.voicedProbability}`);\n * }\n * ```\n */\nexport interface PitchResult {\n  /** Detected pitch in Hz (or null if no pitch detected) */\n  frequency: number | null;\n  /** Confidence score (0-1) */\n  confidence: number;\n  /** Whether audio is voiced */\n  isVoiced: boolean;\n  /**\n   * Probabilistic voiced/unvoiced decision (0-1).\n   * Added in loqa-voice-dsp v0.4.0 with pYIN algorithm.\n   *\n   * Values closer to 1 indicate higher probability of voiced speech.\n   * This provides a \"soft\" decision compared to the binary isVoiced flag.\n   */\n  voicedProbability: number;\n}\n\n/**\n * Configuration options for formant extraction\n */\nexport interface FormantExtractionOptions {\n  /** Sample rate in Hz */\n  sampleRate: number;\n  /** LPC order. Defaults to sampleRate / 1000 + 2. */\n  lpcOrder?: number;\n}\n\n/**\n * Result of formant extraction\n *\n * Note: In loqa-voice-dsp v0.4.0, bandwidths were replaced with a confidence score.\n *\n * @example\n * ```typescript\n * const result = await extractFormants(audioData, 44100);\n *\n * console.log(`F1: ${result.f1} Hz`);\n * console.log(`F2: ${result.f2} Hz`);\n * console.log(`F3: ${result.f3} Hz`);\n * console.log(`Confidence: ${result.confidence}`);\n * ```\n */\nexport interface FormantsResult {\n  /** First formant (F1) in Hz */\n  f1: number;\n  /** Second formant (F2) in Hz */\n  f2: number;\n  /** Third formant (F3) in Hz */\n  f3: number;\n  /**\n   * Confidence score for the formant extraction (0-1).\n   * Added in loqa-voice-dsp v0.4.0, replacing bandwidths.\n   *\n   * Higher values indicate more reliable formant detection.\n   */\n  confidence: number;\n}\n\n/**\n * Configuration options for spectrum analysis\n */\nexport interface SpectrumAnalysisOptions {\n  /** Sample rate in Hz */\n  sampleRate: number;\n}\n\n/**\n * Result of spectrum analysis\n */\nexport interface SpectrumResult {\n  /** Spectral centroid in Hz (brightness measure) */\n  centroid: number;\n  /** Spectral rolloff in Hz (95% energy threshold) */\n  rolloff: number;\n  /** Spectral tilt (slope of spectrum) */\n  tilt: number;\n}\n\n/**\n * Configuration options for HNR (Harmonics-to-Noise Ratio) calculation\n *\n * HNR measures the ratio of harmonic to noise energy in voice,\n * providing a quantitative measure of breathiness.\n *\n * @example\n * ```typescript\n * const options: HNROptions = {\n *   sampleRate: 44100,\n *   minFreq: 75,   // Minimum F0 to search\n *   maxFreq: 500   // Maximum F0 to search\n * };\n * ```\n */\nexport interface HNROptions {\n  /** Sample rate in Hz */\n  sampleRate: number;\n  /**\n   * Minimum fundamental frequency to search in Hz.\n   * Defaults to 75 Hz. Lower values may be needed for very low voices.\n   */\n  minFreq?: number;\n  /**\n   * Maximum fundamental frequency to search in Hz.\n   * Defaults to 500 Hz. Higher values may be needed for very high voices.\n   */\n  maxFreq?: number;\n}\n\n/**\n * Result of HNR (Harmonics-to-Noise Ratio) calculation\n *\n * HNR is the primary acoustic measure of breathiness:\n * - Higher HNR (18-25 dB): Clear, less breathy voice\n * - Lower HNR (12-18 dB): Softer, more breathy voice\n * - Very low HNR (<10 dB): Very breathy or pathological voice\n *\n * @example\n * ```typescript\n * const result = await calculateHNR(audioBuffer, { sampleRate: 44100 });\n *\n * if (result.isVoiced) {\n *   if (result.hnr > 20) {\n *     console.log('Clear voice detected');\n *   } else if (result.hnr < 15) {\n *     console.log('Breathy voice detected');\n *   }\n * }\n * ```\n */\nexport interface HNRResult {\n  /**\n   * Harmonics-to-Noise Ratio in decibels (dB).\n   *\n   * Typical ranges:\n   * - 12-18 dB: Breathy voice (e.g., soft, warm quality)\n   * - 18-25 dB: Clear voice (e.g., bright, crisp quality)\n   * - 0 dB: Returned for unvoiced signals\n   */\n  hnr: number;\n  /**\n   * Detected fundamental frequency in Hz.\n   *\n   * This is the F0 used for HNR calculation.\n   * Returns 0 if signal is unvoiced.\n   */\n  f0: number;\n  /**\n   * Whether the signal is voiced (periodic).\n   *\n   * If false, the signal is either unvoiced (noise, whisper)\n   * or below the voicing threshold.\n   */\n  isVoiced: boolean;\n}\n\n/**\n * Configuration options for H1-H2 amplitude difference calculation\n *\n * H1-H2 measures the difference between the first and second harmonic amplitudes,\n * correlating with vocal weight (lighter vs fuller voice quality).\n *\n * @example\n * ```typescript\n * const options: H1H2Options = {\n *   sampleRate: 44100,\n *   f0: 200  // Optional: provide pre-calculated F0 for efficiency\n * };\n * ```\n */\nexport interface H1H2Options {\n  /** Sample rate in Hz */\n  sampleRate: number;\n  /**\n   * Optional pre-calculated fundamental frequency in Hz.\n   *\n   * If provided, skips F0 detection (faster).\n   * If omitted or set to 0, F0 will be auto-detected.\n   * Use this if you've already called detectPitch().\n   */\n  f0?: number;\n}\n\n/**\n * Result of H1-H2 amplitude difference calculation\n *\n * H1-H2 is a key acoustic correlate of vocal weight:\n * - Higher H1-H2 (>5 dB): Lighter, breathier vocal quality\n * - Lower H1-H2 (<0 dB): Fuller, heavier vocal quality\n * - Moderate H1-H2 (0-5 dB): Balanced vocal weight\n *\n * @example\n * ```typescript\n * const result = await calculateH1H2(audioBuffer, { sampleRate: 44100 });\n *\n * if (result.h1h2 > 5) {\n *   console.log('Lighter voice detected');\n * } else if (result.h1h2 < 0) {\n *   console.log('Fuller voice detected');\n * }\n * ```\n */\nexport interface H1H2Result {\n  /**\n   * H1-H2 amplitude difference in decibels (dB).\n   *\n   * Calculated as: H1_amplitude_dB - H2_amplitude_dB\n   *\n   * Typical ranges:\n   * - >5 dB: Lighter, breathier quality\n   * - 0-5 dB: Balanced\n   * - <0 dB: Fuller, heavier quality\n   */\n  h1h2: number;\n  /**\n   * First harmonic (fundamental) amplitude in dB.\n   *\n   * This is the amplitude at the F0 frequency.\n   */\n  h1AmplitudeDb: number;\n  /**\n   * Second harmonic amplitude in dB.\n   *\n   * This is the amplitude at 2*F0 frequency.\n   */\n  h2AmplitudeDb: number;\n  /**\n   * Fundamental frequency used for calculation in Hz.\n   *\n   * Either the provided F0 or the auto-detected value.\n   */\n  f0: number;\n}\n\n/**\n * Configuration options for VoiceAnalyzer streaming API\n *\n * The VoiceAnalyzer provides stateful pitch tracking with HMM smoothing,\n * ideal for analyzing longer audio clips frame-by-frame with temporal coherence.\n *\n * @example\n * ```typescript\n * const config: VoiceAnalyzerConfig = {\n *   sampleRate: 44100,\n *   minFrequency: 80,\n *   maxFrequency: 400,\n *   frameSize: 2048,\n *   hopSize: 512\n * };\n * ```\n */\nexport interface VoiceAnalyzerConfig {\n  /** Sample rate in Hz (8000-48000) */\n  sampleRate: number;\n  /**\n   * Minimum detectable frequency in Hz.\n   * Defaults to 80 Hz (low male voice).\n   */\n  minFrequency?: number;\n  /**\n   * Maximum detectable frequency in Hz.\n   * Defaults to 400 Hz (high female voice).\n   */\n  maxFrequency?: number;\n  /**\n   * Frame size in samples for analysis.\n   * Defaults to 2048 samples (~46ms at 44100 Hz).\n   * Larger frames provide better frequency resolution but worse time resolution.\n   */\n  frameSize?: number;\n  /**\n   * Hop size in samples between consecutive frames.\n   * Defaults to frameSize / 4 (75% overlap).\n   * Smaller hop sizes provide smoother pitch tracks but more computation.\n   */\n  hopSize?: number;\n}\n\n/**\n * Handle to a VoiceAnalyzer instance\n *\n * This is an opaque handle returned by createVoiceAnalyzer().\n * Use it with analyzeClip(), resetVoiceAnalyzer(), and freeVoiceAnalyzer().\n */\nexport interface VoiceAnalyzerHandle {\n  /** Unique identifier for this analyzer instance */\n  id: string;\n  /** Configuration used to create this analyzer */\n  config: VoiceAnalyzerConfig;\n}\n\n/**\n * Result of analyzing a clip with VoiceAnalyzer\n *\n * Contains an array of pitch results for each frame, plus aggregate statistics.\n *\n * @example\n * ```typescript\n * const result = await analyzeClip(analyzer, audioData);\n *\n * console.log(`Analyzed ${result.frameCount} frames`);\n * console.log(`Median pitch: ${result.medianPitch} Hz`);\n * console.log(`Voiced frames: ${result.voicedFrameCount}`);\n *\n * // Access individual frame results\n * for (const frame of result.frames) {\n *   if (frame.isVoiced) {\n *     console.log(`Frame pitch: ${frame.frequency} Hz`);\n *   }\n * }\n * ```\n */\nexport interface VoiceAnalyzerResult {\n  /**\n   * Array of pitch results for each analyzed frame.\n   * Frames are analyzed with HMM smoothing for temporal coherence.\n   */\n  frames: PitchResult[];\n  /** Total number of frames analyzed */\n  frameCount: number;\n  /** Number of frames detected as voiced */\n  voicedFrameCount: number;\n  /**\n   * Median pitch across all voiced frames in Hz.\n   * null if no voiced frames were detected.\n   * Use this as the primary pitch estimate for the clip.\n   */\n  medianPitch: number | null;\n  /**\n   * Mean pitch across all voiced frames in Hz.\n   * null if no voiced frames were detected.\n   */\n  meanPitch: number | null;\n  /**\n   * Standard deviation of pitch across voiced frames in Hz.\n   * null if fewer than 2 voiced frames were detected.\n   * Lower values indicate more stable pitch.\n   */\n  pitchStdDev: number | null;\n  /**\n   * Mean confidence across all voiced frames (0-1).\n   * null if no voiced frames were detected.\n   */\n  meanConfidence: number | null;\n  /**\n   * Mean voiced probability across all frames (0-1).\n   * Higher values indicate more of the clip was voiced.\n   */\n  meanVoicedProbability: number;\n}\n\n/**\n * Result of processing a complete audio buffer with Viterbi decoding (v0.5.0)\n *\n * Unlike VoiceAnalyzerResult which analyzes frames independently, PitchTrack uses\n * HMM-smoothed Viterbi decoding to find the globally optimal pitch track across\n * all frames. This significantly reduces octave jump errors (from ~8-12% to â‰¤3%)\n * and produces smoother pitch contours.\n *\n * Best suited for offline analysis of complete utterances (typically < 60 seconds).\n * For longer recordings, segment into utterances first.\n *\n * **Note:** Always uses pYIN algorithm regardless of VoiceAnalyzerConfig.algorithm setting,\n * since HMM smoothing requires the probabilistic candidates that only pYIN provides.\n *\n * @example\n * ```typescript\n * const analyzer = await createVoiceAnalyzer({ sampleRate: 44100 });\n * const track = await processBuffer(analyzer, audioSamples);\n *\n * console.log(`Analyzed ${track.frameCount} frames`);\n * console.log(`Median pitch: ${track.medianPitch} Hz`);\n *\n * // Access raw pitch track data\n * for (let i = 0; i < track.pitchTrack.length; i++) {\n *   const pitch = track.pitchTrack[i];\n *   const prob = track.voicedProbabilities[i];\n *   const time = track.timestamps[i];\n *   if (pitch > 0) {\n *     console.log(`t=${time.toFixed(3)}s: ${pitch.toFixed(1)} Hz (${(prob * 100).toFixed(0)}% voiced)`);\n *   }\n * }\n *\n * await freeVoiceAnalyzer(analyzer);\n * ```\n */\nexport interface PitchTrack {\n  /**\n   * Pitch estimates per frame in Hz (0.0 = unvoiced).\n   * This is the globally optimal pitch track computed via Viterbi decoding.\n   * Length matches frameCount.\n   */\n  pitchTrack: Float32Array;\n  /**\n   * Voiced probability per frame [0.0, 1.0].\n   * Higher values indicate higher confidence that the frame contains voiced speech.\n   * Length matches frameCount.\n   */\n  voicedProbabilities: Float32Array;\n  /**\n   * Frame timestamps in seconds from buffer start.\n   * Computed as frame_index * hop_size / sample_rate.\n   * Length matches frameCount.\n   */\n  timestamps: Float32Array;\n  /** Total number of frames analyzed */\n  frameCount: number;\n  /** Number of voiced frames (pitch > 0) */\n  voicedFrameCount: number;\n  /**\n   * Median pitch across voiced frames in Hz.\n   * null if no voiced frames were detected.\n   */\n  medianPitch: number | null;\n  /**\n   * Mean pitch across voiced frames in Hz.\n   * null if no voiced frames were detected.\n   */\n  meanPitch: number | null;\n}\n"]}